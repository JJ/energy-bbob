\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}

\newcommand{\pinp}{\texttt{pinpoint}}
\newcommand{\lik}{\texttt{likwid}}

\begin{document}
\title{Optimizing energy consumption of BBOB fitness functions}

%\author{
%  Juan J. Merelo-Guervós\inst{1}\orcidID{0000-0002-1385-9741} \and Antonio M. Mora\inst{2}\orcidID{0000-0003-1603-9105} \and Mario García-Valdez\inst{3}\orcidID{0000-0002-2593-1114}
%}
% \institute{Department of Computer Engineering, Automatics and Robotics and CITIC University of Granada, Granada, Spain \and
% Department of Signal Theory, Telematics and Communications, University of Granada, Granada, Spain \and
% Department of Graduate Studies, National Technological Institute of Mexico, Tijuana, Mexico\\
% \email{jmerelo@ugr.es, amorag@ugr.es, mario@tectijuana.edu.mx}
% }

\author{ A. U. Thorone \inst{1} \and B. U. Thortwo \inst{2} \and C. U. Thorzree \inst{1}}
\institute{ Department of Paper Publication, Miskatonic University, Arkham, MA, USA \and Department of Miscellaneous Stuff, University of Nowhere, Nowhere, ZZ, USA\\
}
\maketitle

\begin{abstract}

\keywords{Green computing, metaheuristics, energy-aware computing, evolutionary algorithms}
\end{abstract}

\section{Introduction}

We live in an era where obtaining better performance through faster hardware that besides consumes less power is no longer possible. Optimization through software is still possible at many levels, through the use of software engineering techniques or at the evolutionary algorithm level, trying to create better ways of sampling the solution space so that better solutions are found faster. However, energy consumption is also a concern, and has been in the area of evolutionary algorithms for many years already.

% Im not sure about the first sentence, the M4 chip is faster and consumes less power than the M1 chip.
% But I think the point is that the performance gains are not as big as they used to be.
% Maybe make that more explicit? - Mario

\section{State of the art}

\section{Methodology}

We will follow a methodology that has been already used in several other papers \cite{lion24-anon,zig1-anon,zig2-anon,icsoft23-anon}. The tool used for energy profiling is called {\sf pinpoint} \cite{pinpoint}, a multiplatform tool that taps into the RAPL API or, in the case of other platforms or operating systems, different APIs, offering a single command for measuring how much energy a process consumes. Since {\sf pinpoint} runs with a command, synchronization is not an issue, yielding system-wide energy consumption {\em while a process is running}.
% I am confused about the last sentecte, pinpint measutes the energy consumed by one process, but then we talk
% about system-wide energy consumption. - Mario
% Pinpoint gives you both resuts? - Mario
This is why our methodology needs an additional step to register the actual energy consumed by the specific workload we are interested in. In order to do that, in this paper we will follow the same methodology as in other papers such as \cite{wivace23-anon}: measure a common function as baseline; averages obtained here will be subtracted from the time and energy consumed by the functions we want to study later on.
%  Lets see if I got this right, pinpoint measures the energy of the whole process, but we need the energy on one function, so we need to subtract the energy of the rest of the process? - Mario

Effectively, what we will use as baseline is chromosome generation. After all, you need to generate chromosomes to apply any kind of function to them. Time consumed in generation, however, is not critical in an evolutionary algorithm since it is done only once at algorithm startup, so we will measure it together with the energy consumed by the whole system. Besides using this as a baseline, this will give us also a measure of energy consumed by different types of data structures.

% I was seeing a guy in Youtube that was optimizing a hash function for speed in C, he went as far as
% seeing in a very cool website, the assembly code generated by the compiler, and then, he was able to optimize the code with
% those hints. Maybe we should make clear at what level of programming cunstructs we want to reach to optimiza for
% performance. For instance, stating that we are only comparing data types, or compiler switches etc. - Mario

We will perform all experiments in a Linux system {\tt 5.15.0-124-generic \#134~20.04.1-Ubuntu SMP} running in an {\em AMD Ryzen 9 3950X} with 16 cores, although we will be using a single core. The compiler we will use is {\tt g++ 10.5.0}\footnote{This is admittedly not the latest version, but the most modern one that can be run in a Ubuntu 20.04}. All programs have been compiled with the flags {\tt -flto -march=native -O3 -Wall -std=c++2a}, that is, link-time optimization, native architecture, third level of optimization (which is the maximum available, {\tt -O4} might result in worse performance) that instructs the compiler to use all reasonable machine-level optimizations, all warnings, and the C++ 20 standard. In general, what we are telling the compiler is to use its own heuristics to create the best possible code for the architecture we are running on, as well as the most current standard it is able to process \footnote{Which is not the most current one, however. We do not think that the specific standard will have influence on the ranking of results, though.}.

<<evostar.bbob.setup, echo=F, message=F, fig.height=4, fig.cap="Time in seconds for every experiment generating 40K chromosomes with 128,256,512 dimensions, using float or double for every one of them">>=
library(ggplot2)
library(ggthemes)
library(dplyr)

base.variable.data <- read.csv("../data/evostar25-generation-5-Nov-07-30-15.csv")

base.variable.data$size <- as.factor(base.variable.data$size)
base.variable.data$type <- as.factor(base.variable.data$type)
ggplot(base.variable.data, aes(x=PKG, y=seconds, color=type, shape=size)) +
  geom_point() +
  labs(title="Energy consumption of BBOB functions", x="PKG Energy (Joules)", y="seconds") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

base.variable.data %>% group_by(size, type) %>% summarise(mean.seconds=mean(seconds), sd.seconds=sd(seconds), mean.PKG = mean(PKG), sd.PKG = sd(PKG)) -> summary.base.variable.data
@

The code use, which is available from this paper repository (hidden URL) has been adapted from the BBOB code \cite{hansen2010comparing} by using C++ generics to employ the same code for {\sf float} and {\sf double} types for each dimension. Since {\sf doubles} take twice as many bytes as {\sf floats}, simply going through them should have an impact on consumed energy. By default, {\sf BBOB} is programmed for {\sf doubles}; however, using float or double should not have any measurable impact on evolutionary algorithm performance. BBOB looks for a $10^{-8}$ target; this is close to {\sf float} precision (which is 6 digits without round-trip errors, 7 with some errors, 8 attainable depending on the values codified), so we could not really use {\sf float} to represent the dimension of BBOB functions. However, we are using BBOB functions as representative of floating-point fitness function, and assuming the type used to represent function values is a choice that is under control of the researcher. We are also trying to measure what is the difference in energy consumption between using {\sf float} and {\sf double}, so that this enters into consideration when creating an EA experiment. In both cases, we will use the variable-size {\sf vector} data structure from the standard library.

% This is similar to strategies used for reducing the energyy consumption in machile learging,
% where the precision of the data is reduced to save energy.
% But I think in those cases they go from 32 to 16 bits or even 8 bits. - Mario

The other variable we are changing in these experiments is the number of dimensions, which we call {\sf size} here. We will start at 128, and double up to 512. Again, the dimensions used in BBOB stop at 100, but increasing the size will be useful to be able to measure differences in time and energy consumed with more precision.

As is usual, we have generated 40K random chromosomes, using the RNG that consumes the least, {\tt mt19937\_64} \cite{mersennetwister}, and repeated runs for every combination 30 times. The results are shown in Figure \ref{fig:evostar.bbob.setup}. What we see in this figure is that the spread in energy consumption is, as usual, much bigger than the spread in running time, which has been found elsewhere and is expected. The energy a program draws from the system depends on the ambient temperature, as well as the other programs that are running, the {\em split} observed between 10 Joules and around 14 joules for the floating point experiments with size = 512 is not surprising, and is one of the reasons why we are running 30 experiments in the first place. Other than that, what we see is that except for this case, energy consumed is not so different between float and double; while the amount of time it take is, they consume roughly the same, meaning the power drawn by double precision arithmetic is {\em lower} than for single precision. We need to take into account, however, that this is a baseline measurement that will be influenced by the system overhead; we will need to measure the actual functions to confirm these measurements.

% Other papers have observed differences in energy consumption affected by the CPU temperature, I think this is more
% important than the ambient temperature, also the temperature of the CPU increases as it its working  - Mario

<<evostar.bbob.fixed, echo=F, message=F, fig.height=4, fig.cap="Time in seconds for every experiment generating 40K chromosomes with 128,256,512 dimensions, using float or double for every one of them">>=
base.fixed.data <- read.csv("../data/fixed-evostar25-generation-5-Nov-07-37-13.csv")

base.fixed.data$size <- as.factor(base.fixed.data$size)
base.fixed.data$type <- as.factor(base.fixed.data$type)
base.fixed.data$work <- rep("array", nrow(base.fixed.data))

base.variable.data.128 <- base.variable.data[base.variable.data$size == 128,]
base.variable.data.128$work <- rep("vector", nrow(base.variable.data.128))

base.data.128 <- rbind(base.fixed.data, base.variable.data.128)

ggplot(base.data.128, aes(x=PKG, y=seconds, color=type, shape=work)) +
  geom_point() +
  labs(title="Energy consumption of BBOB functions, vector vs. array", x="PKG Energy (Joules)", y="seconds") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

base.fixed.data %>% group_by(size, type) %>% summarise(mean.seconds=mean(seconds), sd.seconds=sd(seconds), mean.PKG = mean(PKG), sd.PKG = sd(PKG)) -> summary.base.fixed.data
@

We are, however, also interested in the actual type we use to represent this number vector if it has any influence in the energy it consumes. C++ includes some fixed-size types, such as {\sf array}. From the point of view of programming, this needs the array size to be constant, which means that you need to generate and compile different sources for every size; this is why we have run an experiment just for size = 128. This is shown in Figure \ref{fig:evostar.bbob.fixed}; colors again represent the type used for every number, while shape varies with the kind of data structures, fixed-sized array as dots, and variable-sized vector as triangles. There is a clear difference between the two, in running time as well as energy consumed, with the former consuming almost one eighth of the joules, and taking almost half of the time. Again, energy consumed by single and double precision is almost the same, differing only in the time it takes. Since the only operation we are applying here is random number generation, this would mean that generating random doubles consumes the same amount of energy than generating single-precision numbers\footnote{Since the RNG used is for 64 bits, it is very likely that what it is actually doing is {\em always} generating that amount of bits and then picking 4 for single-precision {\sf floats}}.

We should emphasize that that this is the baseline measurement. We will measure the energy consumed by a selected sample of BBOB functions next.

\section{Measuring energy consumption of BBOB functions}
\label{sec:measurements}

<<evostar.bbob.functions, echo=F, message=F, warning=F, fig.height=6, out.width="30%", fig.show="hold", fig.cap="PKG energy consumed in every experiment evaluating 40K chromosomes with 128, 256, 512 dimensions, using float or double for every one of them. Please note the $y$ axes have different scales">>=
functions.data <- read.csv("../data/variable-evostar25-bbob-10-Nov-19-10-32.csv")
functions.data <- functions.data[functions.data$work != "none",]
number.of.rows <- nrow(functions.data[ functions.data$size==128 & functions.data$type==" f",])
functions.data$delta.PKG <- 0
functions.data[ functions.data$size==128 & functions.data$type==" f",]$delta.PKG <- functions.data[ functions.data$size==128 & functions.data$type==" f",]$PKG - rep(summary.base.variable.data[ summary.base.variable.data$size == 128 & summary.base.variable.data$type==" f", ]$mean.PKG,number.of.rows)

functions.data[ functions.data$size==256 & functions.data$type==" f",]$delta.PKG <- functions.data[ functions.data$size==256 & functions.data$type==" f",]$PKG - rep(summary.base.variable.data[ summary.base.variable.data$size == 256 & summary.base.variable.data$type==" f", ]$mean.PKG,number.of.rows)

functions.data[ functions.data$size==512 & functions.data$type==" f",]$delta.PKG <- functions.data[ functions.data$size==512 & functions.data$type==" f",]$PKG - rep(summary.base.variable.data[ summary.base.variable.data$size == 512 & summary.base.variable.data$type==" f", ]$mean.PKG,number.of.rows)

functions.data[ functions.data$size==128 & functions.data$type==" d",]$delta.PKG <- functions.data[ functions.data$size==128 & functions.data$type==" d",]$PKG - rep(summary.base.variable.data[ summary.base.variable.data$size == 128 & summary.base.variable.data$type==" d", ]$mean.PKG,number.of.rows)

functions.data[ functions.data$size==256 & functions.data$type==" d",]$delta.PKG <- functions.data[ functions.data$size==256 & functions.data$type==" d",]$PKG - rep(summary.base.variable.data[ summary.base.variable.data$size == 256 & summary.base.variable.data$type==" d", ]$mean.PKG,number.of.rows)

functions.data[ functions.data$size==512 & functions.data$type==" d",]$delta.PKG <- functions.data[ functions.data$size==512 & functions.data$type==" d",]$PKG - rep(summary.base.variable.data[ summary.base.variable.data$size == 512 & summary.base.variable.data$type==" d", ]$mean.PKG,number.of.rows)

functions.data$delta.PKG <- pmax(functions.data$delta.PKG, 0)

ggplot(functions.data[ functions.data$size==128,], aes(x=work, y=delta.PKG, color=type)) +
  geom_boxplot() +
  labs(title="Energy consumption of BBOB functions, length=128", y="PKG Energy (Joules)", x="function") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_y_log10() + theme(legend.position="none")

ggplot(functions.data[ functions.data$size==256,], aes(x=work, y=delta.PKG, color=type)) +
  geom_boxplot() +
  labs(title="Energy consumption of BBOB functions, length=256", y=NULL, x="function") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+ scale_y_log10()+ theme(legend.position="none")

ggplot(functions.data[ functions.data$size==512,], aes(x=work, y=delta.PKG, color=type)) +
  geom_boxplot() +
  labs(title="Energy consumption of BBOB functions, length=256", y=NULL, x="function") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+ scale_y_log10()
@

As mentioned, the Black Box Optimization Benchmark \cite{hansen2010comparing} proposes a set of fitness function that act on floating point chromosomes, 24 of them, grouped in several subsets: separable, low or moderate conditioning, high conditioning or unimodal and finally multimodal functions with weak or "adequate" global structure. Since there are 5 subsets, we have picked up some from each one: {\sf sphere} and {\sf Rastrigin} from the first, {\sf Rosenbrock} from the second, {\sf discus}, {\sf bent\_cigar}, {\sf different\_powers} and {\sf sharp\_ridge} from the third, {\sf schaffers} from the fourth and finally {\sf katsuura} and {\sf schwefel} from the last one. These are simply representative functions chosen randomly. We think they are a good, non-biased representation of the whole set of functions, at least from the point of view of energy consumption.

As in previous papers, we have applied the functions to the 40K chromosomes previously generated. In Figure \ref{fig:evostar.bbob.functions} we show a boxplot of the energy consumed by each function, once the average energy used to generate the chromosomes has been subtracted, and organized by chromosome size, with energy consumed by {\sf float} and {\sf double} shown in different colors. At the left chart we can see that some glyphs for {\sf float} are missing; this is simply because there is basically no difference between the energy consumed by generation and the one needed to compute the function; this is probably due to the fact that they use essentially the same code so that the delta in energy consumption is optimized away; this happens with the {\sf bent\_cigar}, {\sf different\_powers}, {\sf discus} and {\sf schwefels} BBOB functions.

Another interesting pattern is that the range of energy consumption is so big that we had to use a logarithmic scale to represent it. There is a function that is clearly off-scale, which is {\sf katsuura}, consuming energy that is an order of magnitude above the next ones {\sf rastrigin}, {\sf different\_powers}, {\sf schwefel} and {\sf schaffers}. The rest do not consume so much power, and there is even an extreme case, {\sf rosenbrock}, that shows 0 consumption for both {\sf float} and {\sf double} when length=128, and is the only one that shows 0 consumption for double (or for any kind of single-individual data structure) when length = 128.

<<evostar.bbob.fixed.functions, echo=F, message=F, warning=F, fig.height=4, fig.cap="PKG energy consumed in every experiment evaluating 40K chromosomes with 128, float or double. Please note $y$ scale is logarithmic">>=
fixed.functions.data <- read.csv("../data/evostar25-bbob-fixed-12-Nov-08-20-03.csv")
number.of.rows <- nrow(fixed.functions.data[ fixed.functions.data$size==128 & fixed.functions.data$type==" f",])
fixed.functions.data$delta.PKG <- 0
fixed.functions.data[ fixed.functions.data$size==128 & fixed.functions.data$type==" f",]$delta.PKG <- fixed.functions.data[ fixed.functions.data$size==128 & fixed.functions.data$type==" f",]$PKG - rep(summary.base.fixed.data[ summary.base.fixed.data$size == 128 & summary.base.fixed.data$type==" f", ]$mean.PKG,number.of.rows)
fixed.functions.data[ fixed.functions.data$size==128 & fixed.functions.data$type==" d",]$delta.PKG <- fixed.functions.data[ fixed.functions.data$size==128 & fixed.functions.data$type==" d",]$PKG - rep(summary.base.fixed.data[ summary.base.fixed.data$size == 128 & summary.base.fixed.data$type==" d", ]$mean.PKG,number.of.rows)
fixed.functions.data$delta.PKG <- pmax(fixed.functions.data$delta.PKG, 0)

ggplot(fixed.functions.data[ fixed.functions.data$size==128,], aes(x=work, y=delta.PKG, color=type)) +
  geom_boxplot() +
  labs(title="Energy consumption of BBOB functions, length=128, fixed-size data structure", y="PKG Energy (Joules)", x="function") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+ scale_y_log10()
@

It is difficult to claim one individual data structure being more efficient energy-wise than the other. For the smaller size it is more or less true than {\sf float} is better than {\sf double} except for the {\sf sphere} function. When the size of the chromosome is increased, however, there are several cases when that happens, notable {\sf sharp\_edge}; it shows a slight advantage also in the case of {\sf bent\_cigar}, which is clearer for size = 512. However, two of the more {\em heavy-weight} functions, {\sf rastrigin} and {\sf schaffers}, do not show any significant difference for these sizes, leading us to the preliminary conclusion than when there is a choice, choosing single over double precision will lead only gains in very specific cases, namely, the {\sf katsuura} function \cite{katsuura1991continuous}\footnote{Please note that there is an error in the implementation of this function published by BBOB that will prevent it from yielding the correct value. This, however, is unrelated to the amount of energy it consumes or this fact.}.

<<evostar.bbob.compares, echo=F, message=F, warning=F, fig.height=6, out.width="50%", fig.show="hold", fig.cap="PKG energy consumed in every experiment evaluating 40K chromosomes with 128, 256, 512 dimensions, using float or double for every one of them. Please note the $y$ axes have different scales">>=
fixed.functions.data$data.structure <- "Fixed"
functions.data$data.structure <- "Variable"

functions.128 <- rbind(fixed.functions.data[ fixed.functions.data$size==128,], functions.data[ functions.data$size==128,])

ggplot(functions.128[ functions.128$type==" f",], aes(x=work, y=delta.PKG, color=data.structure)) +
  geom_boxplot( position="dodge") +
  labs(title="Energy consumption of BBOB functions, length=128", y="PKG Energy (Joules)", x="function") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_y_log10()+ theme(legend.position="none")

ggplot(functions.128[ functions.128$type==" d",], aes(x=work, y=delta.PKG, color=data.structure)) +
  geom_boxplot( position="dodge") +
  labs(title="Energy consumption of BBOB functions, length=128", y="PKG Energy (Joules)", x="function") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_y_log10()
@

\section{Discussion and conclusions}

In this paper, we have applied a methodology that has been proved to work correctly for this line of work so far, working with interpreted and compiled functions: splitting the measurements in baseline measurements for generation of chromosomes and then measuring functions allows us to check the actual consumption of the functions (or operators in other papers) we are interested in.

This methodology fails, however, when using advanced compilers that look globally at the code and generate the best machine code possible for performance. This shows in the measurements, when the function under measurement is simply folded into the chromosome generation loop using complex machine code instructions so that it is impossible to separate one from the other, resulting in 0 energy consumption for it if it is simple enough (notably in the case of {\sf rastrigin}). This evidently does not mean that the function is not being executed or not consuming anything in the general case; only that we need to change baseline measurements so that the compiler actually generates independent code for those kind of functions and we can measure it.

\section*{Acknowledgements and data availability}

Acknowledgements Taking\\
These number\\
of lines

% This work is supported by the Ministerio espa\~{n}ol de Econom\'{\i}a y
% Competitividad (Spanish Ministry of Competitivity and Economy) under project
% PID2020-115570GB-C22 (DemocratAI::UGR). We are also very grateful to the {\sf zig} community. Source and data available from \url{https://github.com/JJ/energy-ga-icsoft-2023} under a GPL license.

\bibliographystyle{splncs04}

\bibliography{energy,ga-energy,ours,GAs,cplusplus-energy}


\end{document}

