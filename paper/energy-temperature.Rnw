\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\begin{document}
%
\title{Time-related effects in the measurement of energy consumption in evolutionary algorithms}

\author{
 Juan J. Merelo-Guervós\inst{1}\orcidID{0000-0002-1385-9741} \and Gustavo Romero López\inst{1}\orcidID{0000-0002-5498-7512} \and Mario García-Valdez\inst{2}\orcidID{0000-0002-2593-1114}
}
\institute{Department of Computer Engineering, Automatics and Robotics and CITIC University of Granada, Granada, Spain \and
Department of Graduate Studies, National Technological Institute of Mexico, Tijuana, Mexico\\
\email{jmerelo@ugr.es, gustavo@ugr.es, mario@tectijuana.edu.mx}
}

\authorrunning{JJ Merelo et al.}

\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The main issue with the measurement of energy consumption of any kind of algorithm is establishing a methodology that allows an actionable comparison between different implementations in systems that actively implement their own energy-optimization strategies. In this poster we will try to observe the effects of these strategies in energy-measurement experiments by looking at how the sequence or moment in which a specific measurement is taken affects the results. We will then propose statistics summaries that might be more actionable than a simple average.
\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
%
%
%
\section{Introduction}

Unlike performance evaluation, creating greener software implies developing a methodology for measuring energy consumption along any other measure that tries to optimize it. A set of guidelines have been proposed \cite{ardito2019}, but still almost every paper concerned with this topic includes a section that proposes an {\em ad hoc} methodology specific to its workload; for instance, papers trying to minimize energy consumption in evolutionary algorithms by identifying key functions and measuring its consumption \cite{lion24} include a specific tool and a methodology that tries to isolate the energy consumed by just those functions.

But in this and any other issue, the problem is that the system under measure is being actively optimized at different levels, from the hardware to the operating system, and this is happening at different time scales that depend on the workload and other external factors \cite{multicoreCPUs}. These measures will include frequency scaling as well as disconnection of unused core parts. This has been observed, for instance, in \cite{cotta25}, where the author show how the temperature of the CPU increases as the experiments proceed, with higher energy consumption, and propose a {\em cooling} period between experiments so that energy is optimized by staying longer in a low-temperature low-consumption regime.

The time scale of those experiments is a whole algorithm however; in our methodology, presented in \cite{merelo25}, we deal with single floating point functions, with single-experiment runs on a scale that goes from hundreds of milliseconds to tens of seconds. For every combination of parameters, we had observer a high variability in energy observed that can go up to 10\% or even more; variability is not the same for every kind of measurement, so it cannot simply be attributed to noise or low precision.

In this paper we will revisit the data collected in those experiments looking at the effect of active system optimization measures and how they affect the measures collected. We will check if temperature-related effects are observed or evidence of other path or time-related effects. Finally, in view of the evidence, we will propose different ways of comparing different parametrizations to decide on which one is more energy efficient, since central measures, due to the afore mentioned effects, might not be the best way to perform those comparisons.

\section{Methodology, experiments and results}

<<europar25.variables, echo=FALSE, message=F>>=
bbob.fixed <- read.csv("../data/evostar25-bbob-fixed-12-Nov-08-20-03.csv")

bbob.fixed$watts <- bbob.fixed$PKG / bbob.fixed$seconds

library(dplyr)
bbob.fixed <- bbob.fixed %>%
  group_by(type, size, work) %>%
  mutate(order = row_number()) %>%
  ungroup()

# Per type, size, work make a cumulative sum of the seconds passed and put it in a third column
bbob.fixed <- bbob.fixed %>%
  group_by(type, size, work) %>%
  mutate(cumulative_seconds = cumsum(seconds)) %>%
  ungroup()

bbob.variable <- read.csv("../data/variable-evostar25-bbob-10-Nov-19-10-32.csv")
bbob.variable$watts <- bbob.variable$PKG / bbob.variable$seconds
bbob.variable$size <- as.factor(bbob.variable$size)
bbob.variable <- bbob.variable %>%
  group_by(type, size, work) %>%
  mutate(order = row_number()) %>%
  ungroup()

bbob.variable <- bbob.variable %>%
  group_by(type, size, work) %>%
  mutate(cumulative_seconds = cumsum(seconds)) %>%
  ungroup()

bbob.variable <- bbob.variable %>%
  group_by(type, size, work) %>%
  mutate(watts_75 = quantile(watts, 0.75)) %>%
  ungroup()

bbob.variable <- bbob.variable %>%
  group_by(type, size, work) %>%
  mutate(PKG_75 = quantile(PKG, 0.75)) %>%
  ungroup()
@

The general methodology is extensively described in \cite{merelo25}, whose data is freely available and we will be using here. In that paper we measured several black box optimizations functions \cite{hansen2010comparing} used extensively in evolutionary algorithms

\subsection{Conclusion}


\begin{credits}
\end{credits}

\bibliographystyle{splncs04}
\bibliography{energy,ours,GAs}

\end{document}
